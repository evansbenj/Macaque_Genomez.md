# Filtering based on per site coverage

When I analyzed the data using admixfrog, I identified several areas that appeared to be admixed in all species, but these in fact are regions that are duplicated in the nemestrina group but not the rhesus reference genome, so they have higher coverage. higher pi, and higher pairwise divergence (not sure why).  So I am using vcftools to calculate the per site per individual coverage of variable positions.  I wrote a perl script (24_vcftools_descriptivestats_depth_bychr_sqsub_females_and_males.pl) that executes a sbatch script (vcftools_depth_per_site_F_and_M.sh). Both are below:

```perl
#!/usr/bin/perl
# This script will get depth of coverage by site per individual
# Need this before executing:
# export PERL5LIB=/home/ben/projects/rrg-ben/ben/2017_SEAsian_macaques/bin/vcftools/src/perl/

my $commandline = "module load nixpkgs/16.09";
$status = system($commandline);
$commandline = "module load intel/2018.3";
$status = system($commandline);
$commandline = "module load vcftools/0.1.16";
$status = system($commandline);
$commandline=();

#my $gatkpath = "/home/ben/projects/rrg-ben/ben/2017_SEAsian_macaques/bin/GenomeAnalysisTK-nightly-2017-10-07-g1994025/";
my $referencegenome="/home/ben/projects/rrg-ben/ben/2017_SEAsian_macaques/MacaM/MacaM_mt_y.fa";
my $majorpathboth="/home/ben/projects/rrg-ben/ben/2017_SEAsian_macaques/SEAsian_macaques_bam/females_and_males/";

my @chromosomes =("chr01","chr02a","chr02b","chr03","chr04","chr05","chr06","chr07","chr08","chr09","chr10","chr11","chr12","chr
13","chr14","chr15","chr16","chr17","chr18","chr19","chrX");
my @individuals=("bru_PF707","hecki_PF505","hecki_PF643","hecki_PF644","hecki_PF647","hecki_PF648","maura_PF615","maura_PF713","
maura_PM613","maura_PM614","maura_PM616","nem_GumGum_female","nem_Ngsang_sumatra_female","nem_PM1206","nem_PM664","nem_PM665","n
em_Sukai_male","nigra_PF1001","nigra_PF660","nigra_PM1003","nigrescens_PM1011","nigrescens_PM654","tog_PF549","tonk_PF511","tonk
_PF559","tonk_PF563","tonk_PF597","tonk_PF626","tonk_PM592");

foreach my $individual (@individuals){
    foreach my $chromosome (@chromosomes){
	    $commandline="sbatch vcftools_depth_per_site_F_and_M.sh ";
	    $commandline=$commandline.$majorpathboth."FandM_".$chromosome."_BSQR_jointgeno_allsites_filtered_SNPsonly.vcf.gz ";
	    $commandline=$commandline.$majorpathboth."FandM_".$chromosome."_BSQR_jointgeno_allsites_filtered.vcf.gz_".$individua
l."_".$chromosome."_sitedepth ";
	    $commandline=$commandline." ".$individual;
	    print $commandline,"\n\n";
	    $status = system($commandline);
	    $commandline=();
    }
}

```

``` bash
#!/bin/sh
#SBATCH --job-name=vcftools
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --time=1:00:00
#SBATCH --mem=2gb
#SBATCH --output=vcftools.%J.out
#SBATCH --error=vcftools.%J.err
#SBATCH --account=def-ben

module load nixpkgs/16.09
module load intel/2018.3
module load vcftools/0.1.16

vcftools --gzvcf ${1} --out ${2} --indv ${3} --site-depth
```

Turns out neither of these was as good as GATK, so I used that instead.  
```
module load nixpkgs/16.09
module load gatk/3.8
java -Xmx2g  -jar $EBROOTGATK/GenomeAnalysisTK.jar -T VariantsToTable -R /home/ben/projects/rrg-ben/ben/2017_SEAsian_macaques/MacaM/MacaM_mt_y.fa -V FandM_chr04_BSQR_jointgeno_allsites_withpapio_filtered1_SNPsonly.vcf.gz -F CHROM -F POS -GF DP -o GVCF_chr04_gf_DP.table
```


I also wrote an R script to identify all sites whose rolling average of coverage within a sample was greater than 3* the standard deviation plus the mean coverage of that sample.  This was based on inspection of the samples at a range of coverages.  The average cutoff was about 2 times the mean coverage (this ratio varied among samples), which is perfect because it should effectively exclude many of the duplicated regions (which should have two times or higher converage).  

```R
library (ggplot2)
setwd("/Users/Shared/Previously Relocated Items/Security/projects/2017_SEAsian_macaque_genomz/depth_plot")
# get all the files with the site data
files <- list.files(path = ".", pattern = "GVCF_chr03_gf_DP.table")
my_data<-c()
temp<-c()

# read in the data and name the df based on the file name
for(f in 1:length(files)) {
  temp <- read.table(files[f], header = T)
  my_data <- rbind(my_data,temp)
  temp<-c()
}  

# my_data now has coverage per site info for each sample for all sites, genomewide
dim(my_data)
colnamez <- colnames(my_data)

# here is a function to calculate moving averages
# https://stackoverflow.com/questions/743812/calculating-moving-average
moving_fun <- function(x, w, FUN, ...) {
  # x: a double vector
  # w: the length of the window, i.e., the section of the vector selected to apply FUN
  # FUN: a function that takes a vector and return a summarize value, e.g., mean, sum, etc.
  # Given a double type vector apply a FUN over a moving window from left to the right, 
  #    when a window boundary is not a legal section, i.e. lower_bound and i (upper bound) 
  #    are not contained in the length of the vector, return a NA_real_
  if (w < 1) {
    stop("The length of the window 'w' must be greater than 0")
  }
  output <- x
  for (i in 1:length(x)) {
    # plus 1 because the index is inclusive with the upper_bound 'i'
    lower_bound <- i - w + 1
    if (lower_bound < 1) {
      output[i] <- NA_real_
    } else {
      output[i] <- FUN(x[lower_bound:i, ...])
    }
  }
  output
}

# example
# v <- seq(1:10)

# compute a MA(2)
# moving_fun(v, 2, mean)


# make a new dataframe that has the moving average for each sample throughout the 
# genome.  No worries if the window goes across chromosomes
mv_ave_df2 <- c()
for (i in 3:length(my_data)){  
  # calculate the moving average for each column
  moving_average <- moving_fun(my_data[,i], 50, mean)
  # add this to a new dataframe
  mv_ave_df2 <- cbind(mv_ave_df2,moving_average)
  # rename the column to match the sample
  colnames(mv_ave_df2)[i-2] = colnamez[i]
}  

# add chromosome and position data to mv_ave_df2
mv_ave_df3 <- data.frame(mv_ave_df2,my_data$CHROM,my_data$POS)

colnames(mv_ave_df3)[31] <- "CHROM"
colnames(mv_ave_df3)[32] <- "POS"


# calculate mean depth and sd per sample
mean_depth<- c()
sd_depth<- c()

for (i in 3:length(my_data)){
  x<- mean(my_data[,i],na.rm=T)
  mean_depth<-append(mean_depth,x, after = length(mean_depth))
  y<-sd(my_data[,i],na.rm=T)
  sd_depth<-append(sd_depth,y, after = length(sd_depth))
}  

cutoff_vector <- c()


# identify chr and positions in any sample that are >4 sd above that samples mean coverage
# based on the rolling average
# first make a vector with cutoff values for each sample
for (i in 3:ncol(mv_ave_df3)){
  cutoff <- mean_depth[i-2] + 4*sd_depth[i-2]
  cutoff_vector <- append(cutoff_vector,cutoff, after=length(cutoff_vector))
}

mean_over_cuttoff <- mean_depth/cutoff_vector
plot(mean_over_cuttoff)
# give the elements in cutoff_vector some names
names(cutoff_vector) <- c('bru_PF707.DP','download.DP','hecki_PF505.DP','hecki_PF643.DP',
                          'hecki_PF644.DP','hecki_PF647.DP','hecki_PF648.DP','maura_PF615.DP',
                          'maura_PF713.DP','maura_PM613.DP','maura_PM614.DP','maura_PM616.DP',
                          'nem_GumGum_female.DP','nem_Ngsang_sumatra_female.DP','nem_PM1206.DP',
                          'nem_PM664.DP','nem_PM665.DP','nem_Sukai_male.DP','nigra_PF1001.DP',
                          'nigra_PF660.DP','nigra_PM1003.DP','nigrescens_PM1011.DP',
                          'nigrescens_PM654.DP','tog_PF549.DP','tonk_PF511.DP','tonk_PF559.DP',
                          'tonk_PF563.DP','tonk_PF597.DP','tonk_PF626.DP','tonk_PM592.DP' )

# now cycle through each column and identify chr and pos of the bad ones
sub <- NULL
sub <- subset(mv_ave_df3, 
              bru_PF707.DP > cutoff_vector["bru_PF707.DP"]  | 
              download.DP > cutoff_vector["download.DP"] |
              hecki_PF505.DP  > cutoff_vector["hecki_PF505.DP"] |
              hecki_PF643.DP  > cutoff_vector["hecki_PF643.DP"] |
              hecki_PF644.DP  > cutoff_vector["hecki_PF644.DP"] |
              hecki_PF647.DP > cutoff_vector["hecki_PF647.DP"] |
              hecki_PF648.DP> cutoff_vector["hecki_PF648.DP"] |
                maura_PF615.DP > cutoff_vector["maura_PF615.DP"] |
                maura_PF713.DP > cutoff_vector["maura_PF713.DP"] |
                maura_PM613.DP > cutoff_vector["maura_PM613.DP"] |
                maura_PM614.DP > cutoff_vector["maura_PM614.DP"] |
                maura_PM616.DP > cutoff_vector["maura_PM616.DP"] |
                nem_GumGum_female.DP > cutoff_vector["nem_GumGum_female.DP"] |
                nem_Ngsang_sumatra_female.DP > cutoff_vector["nem_Ngsang_sumatra_female.DP"] |
                nem_PM1206.DP > cutoff_vector["nem_PM1206.DP"] |
                nem_PM664.DP > cutoff_vector["nem_PM664.DP"] |
                nem_PM665.DP > cutoff_vector["nem_PM665.DP"] |
                nem_Sukai_male.DP > cutoff_vector["nem_Sukai_male.DP"] |
                nigra_PF1001.DP > cutoff_vector["nigra_PF1001.DP"] |
                nigra_PF660.DP > cutoff_vector["nigra_PF660.DP"] |
                nigra_PM1003.DP> cutoff_vector["nigra_PM1003.DP"] |
                nigrescens_PM1011.DP > cutoff_vector["nigrescens_PM1011.DP"] |
                nigrescens_PM654.DP > cutoff_vector["nigrescens_PM654.DP"] |
                tog_PF549.DP > cutoff_vector["tog_PF549.DP"] |
                tonk_PF511.DP > cutoff_vector["tonk_PF511.DP"] |
                tonk_PF559.DP > cutoff_vector["tonk_PF559.DP"] |
                tonk_PF563.DP > cutoff_vector["tonk_PF563.DP"] |
                tonk_PF597.DP > cutoff_vector["tonk_PF597.DP"] |
                tonk_PF626.DP > cutoff_vector["tonk_PF626.DP"] |
                tonk_PM592.DP > cutoff_vector["tonk_PM592.DP"]
              )

dim(sub)
dim(mv_ave_df3)


to_file <- data.frame(sub$CHROM, sub$POS)

# write to file
write.table(to_file, "chr03_positions_to_exclude.txt", append = FALSE, sep = "\t", dec = ".",
            row.names = F, col.names = F)
```
After removing double quotes, I used these files and vcftools to filter sites again, like this:
```bash
vcftools --gzvcf FandM_chrX_BSQR_jointgeno_allsites_withpapio_filtered1_SNPsonly.vcf.gz --out FandM_chrX_BSQR_jointgeno_allsites_withpapio_filtered2_coverage_SNPsonly.vcf.gz --exclude-positions chrX_positions_to_exclude.txt --recode
```
And for chrX I removed the entire PAR for everyone, because this causes problems for genotyping males
```
vcftools --gzvcf FandM_chrX_BSQR_jointgeno_allsites_withpapio_filtered2_coverage_SNPsonly.vcf.gz.recode.vcf.gz --out FandM_chrX_BSQR_jointgeno_allsites_withpapio_filtered3_coverage_SNPsonly.vcf.gz --chr chrX --from-bp 1 --to-bp 1691775 --recode
```
